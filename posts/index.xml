<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Untitled Thoughts</title>
    <link>/posts/</link>
    <description>Recent content in Posts on Untitled Thoughts</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <copyright>©2020 Ryan Ma</copyright>
    <lastBuildDate>Wed, 04 Mar 2020 18:07:04 -0500</lastBuildDate>
    
        <atom:link href="/posts/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Binary Tree Interview Questions</title>
      <link>/posts/leetcode-binary-tree/</link>
      <pubDate>Wed, 04 Mar 2020 18:07:04 -0500</pubDate>
      
      <guid>/posts/leetcode-binary-tree/</guid>
      <description>Binary Tree A binary tree is a special type of graph which cannot have any cycles. It is very different from binary search tree in term of properties and usage. Binary tree is an efficient way of representing hiearchical data, and its advanced forms are widely used in implementing database indices, sorting algorithms, encoders and decision-making process.
Binary Tree problems are great to test one&#39;s ability to think recursively, since recursion one of the fundamentals of CS.</description>
    </item>
    
    <item>
      <title>Implement LRU Cache</title>
      <link>/posts/leetcode-lru-cache/</link>
      <pubDate>Thu, 27 Feb 2020 08:07:04 -0500</pubDate>
      
      <guid>/posts/leetcode-lru-cache/</guid>
      <description>Problem: 146. LRU Cache
 Design and implement a data structure for Least Recently Used (LRU) cache. It should support the following operations: get and put.
get(key) - Get the value (will always be positive) of the key if the key exists in the cache, otherwise return -1.
put(key, value) - Set or insert the value if the key is not already present. When the cache reached its capacity, it should invalidate the least recently used item before inserting a new item.</description>
    </item>
    
    <item>
      <title>Accessing mutiple Hadoop amespaces with native HDFS client</title>
      <link>/posts/hadoop-multi-namespaces/</link>
      <pubDate>Wed, 05 Feb 2020 22:26:04 -0500</pubDate>
      
      <guid>/posts/hadoop-multi-namespaces/</guid>
      <description>In hadoop we can have multiple namespaces in hdfs-site.xml for HDFS clients. This is achieved by a HDFS feature called: HDFS Federation
Why do you need multiple namespaces This feature is particularly useful when you need to distcp data between multiple highly available remote hadoop clusters. For HA clusters we don&#39;t always know the address of active namenode and it&#39;s not recommanded to hardcode the active namenode address in the distcp command in case of namenode fail over.</description>
    </item>
    
    <item>
      <title>Cloud is not free, Wakeup and SSH to your home computer remotely</title>
      <link>/posts/remote-ssh-home-computer/</link>
      <pubDate>Sun, 10 Mar 2019 17:04:04 -0500</pubDate>
      
      <guid>/posts/remote-ssh-home-computer/</guid>
      <description>Recently I had to run some Q-learning algorithms on my home desktop (running on Ubuntu 18). These workloads, depends on the model and hyper parameters, could take a while to finish. If I’m away from home and still wants to work on this project, my options would be:
  Run the algorithms on my Macbook pro and thus starting a fire somewhere
  Run the algorithms on the cloud, cheapest computation optimized instance costs $0.</description>
    </item>
    
    <item>
      <title>Producitivity Tools</title>
      <link>/posts/producitivity-tools/</link>
      <pubDate>Tue, 27 Feb 2018 22:04:04 -0500</pubDate>
      
      <guid>/posts/producitivity-tools/</guid>
      <description>Updated: 2020-02-01
This blog post is a summary of a team lunch &amp;amp; learn, special thanks to our awesome senior engineers for sharing their knowledge to improve productivity. I will continuously update this post as I learn more, and dedicated blog will be posted if a subject is worth to go in detail
Pandocs Pandocs is a universal document converter. It can converts document formats like Markdown, HTML, PDF, LaTex, **Jupyter Notebook **, and even Microsoft Word.</description>
    </item>
    
  </channel>
</rss>